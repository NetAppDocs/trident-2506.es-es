---
permalink: trident-protect/monitor-trident-protect-resources.html 
sidebar: sidebar 
keywords: manage, authentication, rbac 
summary: Puedes supervisar el estado de los recursos de Trident Protect utilizando kube-state-metrics y Prometheus.  Esto te proporciona información sobre el estado de las implementaciones, los nodos y los pods. 
---
= Monitor Trident protege los recursos
:allow-uri-read: 
:icons: font
:imagesdir: ../media/


[role="lead"]
Puedes utilizar las herramientas de código abierto kube-state-metrics, Prometheus y Alertmanager para supervisar el estado de los recursos protegidos por Trident Protect.

El servicio kube-state-metrics genera métricas a partir de la comunicación con la API de Kubernetes.  Su uso junto con Trident Protect expone información útil sobre el estado de los recursos en su entorno.

Prometheus es un conjunto de herramientas que puede ingerir los datos generados por kube-state-metrics y presentarlos como información fácilmente legible sobre estos objetos.  En conjunto, kube-state-metrics y Prometheus le brindan una forma de monitorear el estado y la salud de los recursos que administra con Trident Protect.

Alertmanager es un servicio que recibe las alertas enviadas por herramientas como Prometheus y las dirige a los destinos que usted configure.

[NOTE]
====
Las configuraciones y la orientación incluidas en estos pasos son solo ejemplos; debe personalizarlas para que se ajusten a su entorno.  Consulte la siguiente documentación oficial para obtener instrucciones y asistencia específicas:

* https://github.com/kubernetes/kube-state-metrics/tree/main["Documentación de kube-state-metrics"^]
* https://prometheus.io/docs/introduction/overview/["Documentación de Prometeo"^]
* https://github.com/prometheus/alertmanager["Documentación de Alertmanager"^]


====


== Paso 1: Instalar las herramientas de monitorización

Para habilitar la monitorización de recursos en Trident Protect, necesita instalar y configurar kube-state-metrics, Promethus y Alertmanager.



=== Instalar kube-state-metrics

Puedes instalar kube-state-metrics usando Helm.

.Pasos
. Agrega el gráfico Helm kube-state-metrics. Por ejemplo:
+
[source, console]
----
helm repo add prometheus-community https://prometheus-community.github.io/helm-charts
helm repo update
----
. Aplique el CRD de Prometheus ServiceMonitor al clúster:
+
[source, console]
----
kubectl apply -f https://raw.githubusercontent.com/prometheus-operator/prometheus-operator/main/example/prometheus-operator-crd/monitoring.coreos.com_servicemonitors.yaml
----
. Crea un archivo de configuración para el gráfico de Helm (por ejemplo, `metrics-config.yaml` ).  Puede personalizar la siguiente configuración de ejemplo para que se ajuste a su entorno:
+
.metrics-config.yaml: Configuración del gráfico Helm de kube-state-metrics
[source, yaml]
----
---
extraArgs:
  # Collect only custom metrics
  - --custom-resource-state-only=true

customResourceState:
  enabled: true
  config:
    kind: CustomResourceStateMetrics
    spec:
      resources:
      - groupVersionKind:
          group: protect.trident.netapp.io
          kind: "Backup"
          version: "v1"
        labelsFromPath:
          backup_uid: [metadata, uid]
          backup_name: [metadata, name]
          creation_time: [metadata, creationTimestamp]
        metrics:
        - name: backup_info
          help: "Exposes details about the Backup state"
          each:
            type: Info
            info:
              labelsFromPath:
                appVaultReference: ["spec", "appVaultRef"]
                appReference: ["spec", "applicationRef"]
rbac:
  extraRules:
  - apiGroups: ["protect.trident.netapp.io"]
    resources: ["backups"]
    verbs: ["list", "watch"]

# Collect metrics from all namespaces
namespaces: ""

# Ensure that the metrics are collected by Prometheus
prometheus:
  monitor:
    enabled: true
----
. Instale kube-state-metrics implementando el gráfico Helm. Por ejemplo:
+
[source, console]
----
helm install custom-resource -f metrics-config.yaml prometheus-community/kube-state-metrics --version 5.21.0
----
. Configure kube-state-metrics para generar métricas para los recursos personalizados utilizados por Trident Protect siguiendo las instrucciones del documento. https://github.com/kubernetes/kube-state-metrics/blob/main/docs/metrics/extend/customresourcestate-metrics.md#custom-resource-state-metrics["Documentación del recurso personalizado kube-state-metrics"^] .




=== Instalar Prometheus

Puedes instalar Prometheus siguiendo las instrucciones del manual. https://prometheus.io/docs/prometheus/latest/installation/["Documentación de Prometeo"^] .



=== Instalar Alertmanager

Puede instalar Alertmanager siguiendo las instrucciones del siguiente enlace: https://github.com/prometheus/alertmanager?tab=readme-ov-file#install["Documentación de Alertmanager"^] .



== Paso 2: Configurar las herramientas de monitorización para que funcionen conjuntamente.

Después de instalar las herramientas de monitorización, deberá configurarlas para que funcionen conjuntamente.

.Pasos
. Integra kube-state-metrics con Prometheus.  Edita el archivo de configuración de Prometheus(`prometheus.yaml` ) y agregue la información del servicio kube-state-metrics. Por ejemplo:
+
.prometheus.yaml: Integración del servicio kube-state-metrics con Prometheus
[source, yaml]
----
---
apiVersion: v1
kind: ConfigMap
metadata:
  name: prometheus-config
  namespace: trident-protect
data:
  prometheus.yaml: |
    global:
      scrape_interval: 15s
    scrape_configs:
      - job_name: 'kube-state-metrics'
        static_configs:
          - targets: ['kube-state-metrics.trident-protect.svc:8080']
----
. Configura Prometheus para que dirija las alertas a Alertmanager.  Edita el archivo de configuración de Prometheus(`prometheus.yaml` ) y agregue la siguiente sección:
+
.prometheus.yaml: Enviar alertas a Alertmanager
[source, yaml]
----
alerting:
  alertmanagers:
    - static_configs:
        - targets:
            - alertmanager.trident-protect.svc:9093
----


.Resultado
Prometheus ahora puede recopilar métricas de kube-state-metrics y enviar alertas a Alertmanager.  Ahora ya puedes configurar qué condiciones activan una alerta y dónde deben enviarse las alertas.



== Paso 3: Configurar alertas y destinos de alertas

Una vez configuradas las herramientas para que funcionen conjuntamente, es necesario configurar qué tipo de información activa las alertas y dónde deben enviarse dichas alertas.



=== Ejemplo de alerta: fallo de copia de seguridad

El siguiente ejemplo define una alerta crítica que se activa cuando el estado del recurso personalizado de copia de seguridad se establece en `Error` durante 5 segundos o más.  Puedes personalizar este ejemplo para que se ajuste a tu entorno e incluir este fragmento YAML en tu `prometheus.yaml` archivo de configuración:

.rules.yaml: Define una alerta de Prometheus para copias de seguridad fallidas.
[source, yaml]
----
rules.yaml: |
  groups:
    - name: fail-backup
        rules:
          - alert: BackupFailed
            expr: kube_customresource_backup_info{status="Error"}
            for: 5s
            labels:
              severity: critical
            annotations:
              summary: "Backup failed"
              description: "A backup has failed."
----


=== Configura Alertmanager para enviar alertas a otros canales.

Puede configurar Alertmanager para que envíe notificaciones a otros canales, como correo electrónico, PagerDuty, Microsoft Teams u otros servicios de notificación, especificando la configuración correspondiente en el archivo `alertmanager.yaml` archivo.

El siguiente ejemplo configura Alertmanager para enviar notificaciones a un canal de Slack.  Para personalizar este ejemplo a su entorno, reemplace el valor de `api_url` clave con la URL del webhook de Slack utilizada en su entorno:

.alertmanager.yaml: Enviar alertas a un canal de Slack
[source, yaml]
----
data:
  alertmanager.yaml: |
    global:
      resolve_timeout: 5m
    route:
      receiver: 'slack-notifications'
    receivers:
      - name: 'slack-notifications'
        slack_configs:
          - api_url: '<your-slack-webhook-url>'
            channel: '#failed-backups-channel'
            send_resolved: false
----